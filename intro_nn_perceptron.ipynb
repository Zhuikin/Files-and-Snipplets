{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perceptron\n",
    "\n",
    "A perceptron is a simple, basic unit for artificial neural networks. It will take a input vector $S = (S_{0}, S_{1},..., S_{i})$ of signals and return a single output signal:\n",
    "\n",
    "$A = \\sum \\limits _{k=0} ^{i} w_{k} \\cdot S_{k} + b$ \n",
    "\n",
    "wehre $w_{k}$ are the weights for the $k-th$ input and $b$ is tha activation bias. The weight vector $W$ and bias $b$ will be learned during training using the perceptron update rule.\n",
    "\n",
    "Typically used for binary classification problems the output signal might be represented in a binary fashion by applying a $sign$ function:\n",
    "\n",
    "$output = \\begin{cases} 1 & \\text{if } \\text{weighted sum} \\geq 0 \\\\ 0 & \\text{otherwise} \\end{cases}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![perceptron](intro_nn_perceptron.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perceptron learning rule\n",
    "\n",
    "During training a prediction made by the perceptron will be compared to the known classification target. If an error was made, the weights $W$ and bias $b$ will be updated (discounted by the learning rate $\\alpha$):\n",
    "\n",
    "$w_{i} = w_{i} + \\alpha (target - output) \\cdot x_{i}$\n",
    "\n",
    "$b = b + \\alpha (target - output)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single layer of perceptrons as a matix\n",
    "\n",
    "Consider now a single layer of $j$ perceptrons, all taking the same inputs $S$ and each outputting a signal $A_{j}$ (which we might for example, thinking back to the Q-Learner, interpret as the confidence, that the action $A_{j}$ should be taken, given the input state $S$).\n",
    "\n",
    "Each perceptron will have it's own weight vektor $W_{j}$ forming (row-wise) the matrix:\n",
    "\n",
    "$ W = \\begin{pmatrix} W_{0, 0} & W_{0, 1} & \\cdots & W_{0, i} \\\\ \n",
    "W_{1, 0} & W_{1, 1} & \\cdots & W_{1, i} \\\\ \n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\ \n",
    "W_{j, 0} & W_{j, 1} & \\cdots & W_{j, i} \\end{pmatrix}$\n",
    "\n",
    "while the vector $B = ( B_{0}, B_{1} \\cdots B_{j} )$ holds the bias for each output.\n",
    "\n",
    "With that we can now for a set of $i$ inputs $S = ( S_{0}, S_{1} \\cdots S_{i} )$ and $j$ outputs $A = ( A_{0}, A_{1} \\cdots A_{j} )$ evaluate the entire layer at once:\n",
    "\n",
    "$ W \\cdot S + B = \\begin{pmatrix} W_{0, 0} & W_{0, 1} & \\cdots & W_{0, i} \\\\ \n",
    "W_{1, 0} & W_{1, 1} & \\cdots & W_{1, i} \\\\ \n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\ \n",
    "W_{j, 0} & W_{j, 1} & \\cdots & W_{j, i} \\end{pmatrix} \\cdot\n",
    "\\begin{pmatrix} S_{0} \\\\ S_{1} \\\\ \\vdots \\\\ \\vdots \\\\ S_{i} \\end{pmatrix} +\n",
    "\\begin{pmatrix} B_{0} \\\\ B_{1} \\\\ \\vdots \\\\ B_{j} \\end{pmatrix} =\n",
    "\\begin{pmatrix} A_{0} \\\\ A_{1} \\\\ \\vdots \\\\ A_{j} \\end{pmatrix}\n",
    "$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![layer](intro_nn_perceptron_one_layer.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9ccc6a538a0da06e903987474074493caf94daa74ed2a90c8f8989e97d2dea89"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
